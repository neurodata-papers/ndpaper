<!DOCTYPE html>
<html>
    <head>    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>OCP</title>

    <!-- Bootstrap -->
    <link href="Web/css/bootstrap.min.css" rel="stylesheet">	    


    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      @import url(http://fonts.googleapis.com/css?family=Varela+Round:regular,italic,bold,bolditalic);
      @import url(http://fonts.googleapis.com/css?family=Raleway:regular,italic,bold,bolditalic);

      body {
        font-family: 'Varela Round'; /*  */
        background-color: #fff;
        position: absolute;
        left: 20px;
        width: 640px;

      }
      h1, h2, h3, h4, h5, h6 {
        font-family: 'Raleway';
        font-weight: 400;
        margin-bottom: 0;
      }
      p.small {
        font-variant: small-caps;
      }
      .container {
        width: 500px;
        font-size: small;

      }
      </style>

    </head>
    <body>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it/5.0.2/markdown-it.js" charset="utf-8"></script>
<div id="md-raw">

# Open Connectome Project: Reverse Engineering the Brain One Synapse at a Time

## by, everyone
---


<div class=container>
<h1>Abstract</h1>

Recent technical progress allows neuroexperimentalists to collect ever more detailed and informative anatomical and physiological data from brains of all sizes.  These datasets span experimentally accessible spatiotemporal scales, ranging from nanometer to meter, and millisecond to monthly sampling rates.  In classical neuroscientific experimental paradigms, it was feasible for neuroscientists to draw their results on paper. In contrast, many modern neuroscientific experimental paradigms break the classic data analysis workflow.  In particular, these large datasets create significant challenges for our community at every step of the data analysis pipeline: (0) infrastructure, (1) storage, (2) exploration, (3) pre-processing, (4) parsing, and (5) analyzing.


*NeuroData* has been developed to lower the barrier to entry into big data neuroscience. We have designed and built a computational infrastructure to enable petascale neuroscience.  This includes our flagship project, the *Open Connectome Project*. Our infrastructure enables anyone in the world with internet access to visualize, download, analyze, upload, or otherwise interact with a large number public datasets.  Moreover, all the results obtained using the NeuroData infrastructure are fundamentally reproducible and extensible.  We demonstrate the utility of these tools via two serial electron microscopy case studies.  First, we have reproduced all of the quantitative results from a recent landmark EM paper that included a saturated annotation of all objects. Second, we have tested a novel hypothesis about the distribution of synapse locations in cortex on a complementary dataset.  Via the *NeuroData* infrastructure, the answers to these tools are both fully reproducible, and extensible to any other datasets that hold the answers to these questions. Moreover, anybody else can now also do neuroscience at scale, regardless of their background, computational resources, and expertise.

</div>

<!-- We are in the process of scaling up the number of datasets, the range of experimental modalities, and the Web-services we enable. Work underway will provide pre-packaged cluster environments—1-click deployable on local or commercial cloud computing infrastructures—so that others can replicate and modify our services internally. All of our code and data are available online at neurodata.io, in accordance with open science standards.  -->


<h1>Introduction</h1>

<h3>Opportunity</h3>

<p>Data from experimental neuroscientists can now be collected at astonishing rates across a wide variety of experimental paradigms, ranging from serial electron microscopy to calcium imaging to multimodal magnetic resonance imaging.  In each of the different paradigms, the raw data are typically collections of relatively small (e.g., 4 megabyte, MB) two-dimensional (2D) images.  These 2D images can be montaged into large 2D planes, or larger 3D volumes, 3D+time movies, and/or 3D+color multispectral data.  These data have troves of information about brains lurking within, waiting to be extracted by the community.  </p>


<p>The focus of this work is serial electron microscopy (EM) data, a relatively novel yet important experimental modality, as it is unique in its ability to reveal nanoscale anatomical structure [@Denk04,etc.].  Such datasets can obtain terabytes (TB) a day; multiple such datasets already exist with ~100 TB (personal communication, Josh Morgan and Wei-Chung Allen Lee).  
And with the new IARPA program dubbed MICrONS, a 4-10 petabyte (PB) dataset is planned to be collected in the next three years [@MICrONS].  
Moreover, because it is single channel (one color) and single time step (anatomical), addressing big data analysis on these datasets will be a crucial first step prior to addressing big data analysis on multispectral and temporal data.  </p>


<h3>Challenge</h3>


*Unfortunately, existing computational ecosystems for data analysis are ill equipped to meet the challenges that arise with conducting science on terascale data.* Specifically, we divide the data analysis requirements into six components, each of which faces unique challenges:
1. Infrastructure: standard laboratory workstations or even supercomputers cannot manage and manipulate many terabytes of data.  
2. Storage: data cannot be read and written at scale even locally, much less remotely.  
3. Exploration: tools to interactively visualize the data require the whole volume fit into RAM, which is not possible.  
4. Pre-Processing: software requires manual intervention, which becomes a significant bottleneck when there are literally millions of images. 
5. Parsing: manually parsing the scene can be even more time consuming due to the massive number of  voxels (for example, 10<sup>15</sup>). 
6. Analyzing: Analyzing the resulting data derivatives, be they shapes or graphs or both, benefits from tools that natively operate on those data types, with statistical guarantees.



Further, if one group does manage to overcome those barriers, reproducing and extending their results  is typically impossible without collecting a new dataset; an unacceptably costly endeavor.  Science is fundamentally a collective endeavor, where each result builds upon the previous works by "standing on the shoulders of giants" [@Turnbull59].  With small data, the full analysis can easily be reproducible: deposit the digital data in a data repository (like FigShare) and the code in a code repository (like GitHub).  When the data are large, and the analyses are complicated however, neither of those actions are sufficient.  In terms of data, FigShare and related data repositories typically cannot accept many terabytes of data per project.  Even if they did, each researcher would have to make a number of decisions that would impact the utility of the data: in what format would the data be deposited? what metadata to include?   And even then, how would other researchers access it? Would they have to download the whole dataset, or would there be some search mechanism?

Sharing the code can be similarly complicated for big data, as the computations often involve clusters, which are not yet "turn-key"; rather, remain they remain highly specialized.  Therefore, reproducible ones analysis from the code requires access either to the original author's computing environment, or a replica of it.

Finally, science proceeds not merely be reproducing previous results, but by extending them.  Therefore, to the extent that one can make relatively minor changes to a previous analysis, re-using as much infrastructure as possible, the barrier to extend previous results to novel findings can be significantly lowered.




<h3>Action</h3>


We have built the *NeuroData* computational ecosystem to address the above described problems,  therefore enabling reproducible and extensible neuroscience regardless of scale. In particular, we have made a large number of big neuroscience datasets open access.  By storing all of the datasets, regardless of scale and modality, in a common framework, a single infrastructure is sufficient for analysis on each of the datasets.  This includes each of the above five "analysis" steps (storage, exploration, pre-processing, parsing, and analyzing).  Moreover, all of the code we have developed is open source, and divided into two different components: backend and frontend. The backend code is deployed on our Web-accessible data cluster.  This means that anybody can store and explore existing datasets, or contribute new ones.  For the frontend code, we have configured an Amazon Machine Instance (AMI) that can run any of the remaining steps (pre-processing, parsing, and analyzing).  This means that anybody can create an Amazon account, boot this system, and run any of our analyses, and get the same results.  


We have used this infrastructure in two different case studies.  In the first, we have reproduced all of the ~50 quantitative claims made by a  recent landmark EM paper [@Kasthuri15], including a claim about the probability of synapsing given spatial proximity.  Second, we used a significantly larger dataset (~10 TB) from another recent landmark EM paper [@Bock11].  From that data, we characterize the three dimensional distribution of ~10 million synapses in mammalian cortex.  While the scale of the data introduces certain computational complexities, it also enables us to achieve unprecedented clarity in the results.  Crucially, for both of these case studies, all analysis are 100% reproducible and extensible for anybody with internet access.




<h3>Resolution</h3>

<p>
Our infrastructure has therefore enabled  reproducing previous anatomical results on big data, as well as  produced quantitative answers to a fundamental neuroanatomical question with a sufficient sample size to leave little room for doubt about the veridicality of this summary statistic, at least on these data.  
Perhaps more important than the particular questions we have address in this work, is the capability we have enabled for the greater neuroscience community.  Anybody---professional or citizen scientists alike---can use our services to learn about the brain.  Power users can modify our scripts to answer different questions, and developers could add additional capabilities for the community.  We therefore hope to contribute to ushering in a new standard for big data neuroscience.
</p>


<h1>Results</h1>

All of the results presented herein are fundamentally reproducible; the data used to generate each table and figure panel can be reproduced by running a single Python script located in http://paper.neurodata.io.  Figures are made using R's ggplot2; each R script for generating the figures is also available from the above URL.  We have designed all of our supports to support three different types of data: images, annotations, and graphs.  Because the focus of this manuscript is on electron microscopy data, for which scant graphs are currently available, we focus the results on images and annotations.  However, see http://neurodata.io/graphs for more details about graph processing and analytics.


## Infrastructure

.TODO[@kl: make figs]

Our infrastructure is designed for fast reading and writing of terabytes of data.  Figure~\ref{fig:infra} shows the read and write speeds for our system, including writing both images and annotations.  Although we operate our own data cluster for the vast majority of our public facing data, we have built an Amazon Machine Instance (AMI) that is pre-configured with all the necessary software; all the results presented in this section are using that AMI, and are therefore wholly reproducible.


Importantly *NeuroData* is designed to easily enable reading or writing of arbitrarily sized image tiles or cubes using a single URL or Python call.  For example, to download the image in Figure~\ref{fig:infra}(A), one can simply type http://openconnecto.me/ocp/ca/kasthuri11/image/xy/0/9000,9500/12000,12500/50/ into any browser bar.  In contrast, if one desired to download an arbitrary image at an arbitrary zoom from data stored in files in a typical hierarchical folder system, one would have to write code to navigate to the right files, open them up, possibly downsample them, and stitch them back together.  Our infrastructure has built in support for all of that, significantly both speeding up and easing access to the data; Figure~\ref{fig:infra}(B) shows reading speed as a function of cube size, showing that reading speed is linear in the size of the data.    Because it is easy to implement different services for reading data into different file types, we have built support for a variety of exceptional 3rd party tools, to complement this ecosystem.  This includes ITK-SNAP \cite{XXX}, CATMAID \cite{XXX}, Viking \cite{XXX}, Knossos \cite{XXX}, VAST \cite{XXX}, and BigDataViewer \cite{XXX}.


Naively using the above described system, however, has the drawbacks just mentioned.  Therefore, we built *TileCache* which serves as a content distribution network.  More specifically, TileCache fetches the data and parses it into the appropriate format (e.g., a set of png files), and also pre-fetches data that it expects you will want next.  The implication is that for 1028x1028 frames, TileCache can load 60 frames per second in a streaming fashion (Figure~\ref{fig:infra}(C)).  In other words, using TileCache, Neurodata enables video rate data streaming.  And because TileCache is a stand-alone service, it can easily be deployed locally, thereby further eliminating the data transfer speeds to get data local to the machine on which you want the data.




Writing to our spatial database would be more time consuming than simply uploading to a file server, because we re-write the data into a format that is efficient for reading.  We combat that by using a distributed database management system (such as Cassandra), that automatically distributes writing jobs across nodes.  We further speed up writing by compressing the data using Blosc, a multi-threaded library.  The result is that we can write data in compressed cubes in time linear with the data size (Figure~\ref{fig:infra}(D)).  The implications are the writing a terabyte of data on a single node take XXX hours.  

Writing annotations is a bit more complicated than simply writing images because we store annotations as objects with associated metadata, such as what type of object is it (e.g., synapse), who labeled it, what time, with what confidence, etc. Because each write object is therefore relatively small, and write speeds are determined largely by the number of files to write, we built NeuroBlaze, which caches writes in memory, only writing to disk a single large file joining all the small writes.  This enables us to write  annotations extremely efficiently (Figure~\ref{fig:infra}(E)).  

<div class="container">
Figure 1: Reading and writing to the database must be sufficiently fast to not be the primary bottleneck.  (A) OCP can write images faster than most current data collection speeds, meaning that data can be sent directly to OCP without requiring a large local storage buffer.  (B) Reading images from OCP using the tilecache is sufficiently fast to view the data at video rates. (C) Unlike many other scientific big data efforts, neuroscience requires many fast annotation writes.  NeuroBlaze writes annotation objects much more efficiently than our image writing by caching the writes in memory. (D) Although current EM derived neurographs are relatively small, and there are few, we expect that to change dramatically in the coming years. Therefore, we have built infrastructure to support downloading a large number of large graphs efficiently.
</div>


## Databases

We used the above described infrastructure to build a number of image and annotation databases to support a variety of operations.  Table~\ref{tab:images}(A) lists the  public EM datasets that we are currently hosting, as well as several key properties of each.  Those properties that are required for efficient access of the data are stored in a Django data model, the remaining properties are stored in our custom Laboratory Information Management System called ndlims.  The image data in Table~\ref{tab:images}(A) comprise approximately 100 terabytes (TB) of image data .TODO[@j6: make python function that builds html/md/css compliant table to include, and sums up total storage].  By storing disparate datasets in the same format, anybody can access and analyze many different datasets with the same functionality and syntax.  



To complement the image databases, we also created an annotation database management system called RAMON \cite{GrayRoncal15b}.  We have used this infrastructure to store multiple manually and automatically generated dense volumetric annotations, listed in Table~\ref{tab:anno}(B).  

Supplementary Table~\ref{tab:API}(A) lists all the functionality currently supported on all of the image databases, and Supplementary Table~\ref{tab:API}(B) lists the functionality on all of the annotation databases.  Importantly, we have both MATLAB (called CAJAL) and Python (called ndio) bindings to support all of that functionality, without the user having to learn a new programming interface. 


<div class="container">
Table 1: List of all current (as of 12/31/15) Open Connectome Project Datasets.  Table 1A lists the *image* datasets and associated metadata.  Table 1B lists the volumetric *annotation* datasets, derived from a subset of the image datasets. Table 1C lists the *graph* datasets, derived from the annotation datasets.
</div>

## Explore

Interactively exploring these massive data requires tools that can automatically scale appropriately and efficiently.  For this reason, we have developed a number of *NeuroData Explorers*.   For images, we designed *Image Explorer* (IX; http://ix.neurodata.io).  It is the only visualization engine that has the following functionality: (i) loads data remotely from a server, (ii) works in Web-browsers, including mobile, (iii) natively supports volumetric annotations with associated metadata.  In particular, as Figure~\ref{fig:explorer}(A) demonstrates, one can visualize  electron microscopy data, and annotations on top (using one of many color blends), and also see the metadata associated with each annotation object.   Incidentally, IX also supports multiple color channels and time-series data, meaning that one can use it to overlay multiple disparate modalities.

To complement IX, we also built *Graph Explorer* (GX; http://gx.neurodata.io) and *Vector Explorer* (VX; http://vx.neurodata.io) (Figure~\ref{fig:explorer}(A) and (B)).  These two explorers enable users to upload either graphs or tabular data (where rows are subjects and columns are features, as in a csv file).  Both of these tools provide very basic functionality for graphs or tabular data that we find useful for exploring the relevant data types, including different normalization schemes, outlier detection, distributions of various features, and clustering.

<div class="container">
Figure 2: Exploring the data is a crucial first step, regardless of the data type, therefore, we have built data explorers for images, vectors, and graphs. (A) Image Explorer enables a Google Maps like interface to pan in X, Y, Z, and time, zoom, and overlay annotations, blend colors, and more.  (B) Graph Explorer facilitates computing various graph statistics, community detection, and plotting for arbitrary uploaded or generated graphs. (C) Vector Explorer allows the user to upload a csv file and do basic exploratory data analysis. 
</div>

## Pre-process

Pre-processing of data, a process that converts raw data into a format ready for parsing, is an incredibly important and tedious task for essentially any data analysis task.  In electron microscopy, we can divide pre-processing into three steps: (i) geometric or (ii) chromatic alterations, and (iii) registration to a template or atlas. A number of groups are actively developing very nice geometric pre-processing packages, including Terastitcher \cite{XXX} and Saalfeld's Fiji plug-in \cite{XXX}.  We therefore have focused our development efforts on the next two steps.  We built both a 2D and 3D chromatic pre-processing tool. The 2D tool, called *Distriubuted Multigrid Poisson* (DMG) addresses stitching artifacts from montaging multiple 2D tiles into a single large plane (Figure~\ref{fig:pre}(A)).  The 3D tool, called *Gradient Domain Fusion* (GDF) addresses histogram artifacts from different exposure times which can cause a strobe light effect across axial slices of the data (Figure~\ref{fig:pre}(B)).  Both tools were designed to stream the data efficiently, meaning that they can both operate on arbitrarily large data (Figure~\ref{fig:pre}(C)).  

After addressing both geometric and chromatic aberrations, it is often desirable to align the image stack to a reference image, possibly in a different modality.  *ndreg* is our Python tool that sub-serves that functionality.  Because there does not yet exist public data set with both EM and another modality to align, we have aligned a whole mouse brain CLARITY image with the Allen Reference Atlas (Figure~\ref{fig:pre}(D)).  Despite the fact that the two images were collected at different scales, and using different modalities, we are able to successfully and automatically align the two using *ndreg*.

<div class="container">
Figure 3: *NeuroData* provides fully automatic terascale tools for both 2D & 3D chromatic adjustments, as well as nonlinear volume registration.  (A) DMG Chromatic pre-processing adjusts for aberrations induces by data collection and/or geometric stitching. (Bi) 2D image before DMG. (Bii) 2D image after DMG.  (Biii) XZ projection of data before GDF. (Biv) XZ projection after GDF.  (Bv) Time per image size (red) and number of slices (black) on a particular AMI.  (C) Volume registration.
</div>

## Parse


Given that the data has been sufficiently pre-processed, we now proceed to parse the scene, assigning voxels with semantic labels.  Parsing the scene involves three steps: (i) manually labeling a subset of training data, (ii) training and evaluating various machine vision algorithms on appropriate performance metrics, and (iii) deploying the algorithm (and parameter) of choice at scale.  For manual labeling, we built *manno*, which essentially wraps ITK-SNAP by downloading image data from *NeuroData* in the appropriate format, uses ITK-SNAP to label a number of voxels, and then saves and uploads the annotations to the appropriate annotation database (with metadata; see Figure~\ref{fig:parse}(A) for an exemplar slice).  Given the training labels, *macho* uses ilastik to perform pixel level classification \cite{XXX}, and then custom morphological operations to detect objects.  We choose the operating point the maximized F1-score on a held-out dataset located in a volume separated from the training volume (Figure~\ref{fig:parse}(B)).  Finally, we deployed our chosen classifier on the full dataset, yielding ~10 million putative synapse detections (Figure~\ref{fig:parse}(C) shows approximately 1% of them).



<div class="container">
<div><img src="http://docs.neurodata.io/ndintro/images/rob2.png" alt="Drawing" style="height: 250px;"/></div>
Figure 4: 

</div>


## Analyze

Analysis has relatively task specific requirements, although analysis of anatomical data has some shared features.  In particular, from anatomy data, the kinds of questions typically include space, shape, and graphs.  Spatial statistics is a relatively developed branch of statistics, and we therefore can rely on previously developed tools [@Cressie15].  The statistics of shape is a much more recent subdiscipline in statistics [@Younes10], partially because it depends on the mathematics of nonlinear manifolds, which are still being actively developed [@Chikuse03].  




A parsed scene is the data derivative upon which neuroscientific questions can be directly applied.  We focused on two particular questions.  First, a question previously addressed in the literature: To what extend does Peter's Rule hold amongst neurons in mammalian cortex.  To address this question



<h1>Case Studies</h1>


<h2>Reproducing Quantitative Claims from Previous Big Neuroscience Data Paper</h2>


<h2>Discovering the Spatial Distribution of Synapses</h2>



<h1>Methods</h1>



---

<h1>Appendix</h1>



<div class="container" >
Supplementary Figure 1: Infrastructure supporting all components of the OCP data analysis pipeline.  (A) The OCP Data Cluster. (B) Software running on the backend. (C) Software running on the frontend. 
</div>






	</div>

	<script type="text/javascript">
        m = markdownit({
                html: true,
                linkify: true,
                typographer: true
        });
        document.getElementById('md-raw').innerHTML = m.render(
                document.getElementById('md-raw').innerHTML);
     </script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/Web/js/bootstrap.min.js"></script>
    </body>
</html>
 
