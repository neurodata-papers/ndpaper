<!DOCTYPE html>
<html>
    <head>    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>OCP</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">	    


    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      @import url(http://fonts.googleapis.com/css?family=Varela+Round:regular,italic,bold,bolditalic);
      @import url(http://fonts.googleapis.com/css?family=Raleway:regular,italic,bold,bolditalic);

      body {
        font-family: 'Varela Round'; /*  */
        background-color: #fff;
        position: absolute;
        left: 20px;
        width: 640px;

      }
      h1, h2, h3, h4, h5, h6 {
        font-family: 'Raleway';
        font-weight: 400;
        margin-bottom: 0;
      }
      p.small {
        font-variant: small-caps;
      }
      .container {
        width: 500px;
        font-size: small;

      }
      </style>

    </head>
    <body>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it/5.0.2/markdown-it.js" charset="utf-8"></script>
<div id="md-raw">

# Open Connectome Project: Reverse Engineering the Brain One Synapse at a Time

## by, everyone
---

<div class=container>
<h1>Abstract</h1>

Recent technical progress allows neuroexperimentalists to collect ever more detailed and informative anatomical and physiological data from brains of all sizes.  These datasets span experimentally accessible spatiotemporal scales, ranging from nanometers to meters, and millisecond to monthly sampling rates.  In classical neuroscientific experimental paradigms, it was feasible for neuroscientists to draw their results on paper. In contrast, many modern neuroscientific experimental paradigms break the classic data analysis workflow.  In particular, these large datasets create significant challenges for our community at every step of the data analysis pipeline: (i) store, (ii) explore, (iii) pre-process, (iv) parse, (v) analyze, all of which needs to happen in a petascale infrastructure.


*NeuroData*, has been developed to lower the barrier to entry into big data neuroscience. We have designed and built a computational infrastructure to enable petascale neuroscience.  This includes our flagship project, the *Open Connectome Project*. Our infrastructure enables anyone in the world with Internet access to visualize, download, analyze, upload, or otherwise interact with a large number public datasets.  We demonstrate the utility of these tools via two serial electron microscopy use cases.  First, we have reproduced a previous finding, that neuron's do not strictly adhere to Peter's rule. Second, we have tested a novel hypothesis about the distribution of synapse locations in cortex.  Via using the *NeuroData* infrastructure, the answers to these tools are both fully reproducible, and extensible to any other datasets who hold the answers to these questions. Moreover, anybody else can now also do neuroscience at scale, regardless of their background and computational resources and expertise.
</div>

<!-- We are in the process of scaling up the number of datasets, the range of experimental modalities, and the Web-services we enable. Work underway will provide pre-packaged cluster environments—1-click deployable on local or commercial cloud computing infrastructures—so that others can replicate and modify our services internally. All of our code and data are available online at neurodata.io, in accordance with open science standards.  -->


<h1>Introduction</h1>

## Opportunity

Data from experimental neuroscientists can now be collected at astonishing rates across a wide variety of experimental paradigms.  In each of the different paradigms, the raw data are typically collections of relatively small (e.g., 4 megabyte, MB) two-dimensional (2D) images.  These 2D images can be montaged into large 2D planes, or larger 3D volumes, 3D+time movies, and/or 3D+color multispectral data.  These data have troves of information about brains lurking within, waiting to be extracted by the community.  

Of particular interest (and scale) in neuroscience is high-resolution serial electron microscopy [@Denk04,etc.].  Such datasets can obtain terabytes (TB) a day; multiple such datasets already exist with ~100 TB (personal communication, Josh Morgan and Wei-Chung Allen Lee).  And with the new IARPA program dubbed MICrONS, a 4-10 petabyte (PB) dataset is planned to be collected in the next three years [@MICrONS].  Therefore, pipelines for analysis of massive high-dimensional volumes is of paramount importance.

## Gap

Unfortunately, existing computational ecosystems for data analysis are ill equipped to meet the challenges that arise with petascale data.  Further, if one group does manage to overcome those barriers, reproducing and extending their results  is typically impossible without collecting a new dataset; an unacceptably costly endeavor.



## Challenge

We consider six key components necessary for reproducible and extensible data analysis of any type at any scale: (1) hardware and software infrastructure upon which everything else runs,  (2) storage to enable efficient read and write access to the data, (3) exploratory tools to visualize the data in various ways, (4) pre-processing tools, to address geometric and chromatic aberrations, as well as co-register data to reference templates and atlases, (5) tools to parse the scene, semantically labeling (a subset of) voxels, and finally (6) analysis tools to make sense of the parsed scene.  The classic data analysis pipeline with which most neuroscientists are familiar are inadequate in each of these steps.

Specifically, (1) standard laboratory workstations or even supercomputers cannot manage and manipulate many terabytes of data.  (2) Data cannot be read and written at scale even locally, much less remotely.  (3) Tools to visualize the data require the whole volume fit into RAM, which is not possible.  (4) Pre-processing tools require manual intervention, which becomes a significant bottleneck when there are literally millions of images.  (5) Manually parsing the scene is at least as arduous due to the fact that there are $\sim 10^{15}$ voxels. (6) Analyzing the resulting data derivatives, be they shapes or graphs or both, benefits from tools that natively operate on those data types, with statistical guarantees.


## Action

We have built the *NeuroData* computational ecosystem to address each of the six components mentioned above, therefore enabling reproducible and extensible neuroscience regardless of scale. All of our code is open source, and all of the data from which we discover scientific answers is open access.  Moreover, we have deployed a prototype system on our Web-accessible data cluster, which means that anybody can use any of the services we have built free of charge.

For each component of the system, our design was based on achieving sufficient scalability and automation to answer neuroanatomical questions.  Our resulting decisions inevitably led to trade-offs with respect to quality and capabilities.  The results, however, enabled us to answer certain questions much more efficiently and reproducibly than would have otherwise been possible. In particular, we reproduced a recent result regarding Peter's rule in somatosensory cortex [@Kasthuri15], and assessed, for the first time, the three-dimensional distribution of synapses in visual cortex from a different dataset [@Bock11]. 


## Resolution


Our infrastructure has therefore enabled  reproducing previous anatomical results on big data.  Moreover, we have produced quantitative answers to a fundamental neuroanatomical question with a sufficient sample size to leave little room for doubt about the veridicality of this summary statistic, at least on these data.  

Perhaps more important than the particular questions we have address in this work, is the capability we have enabled for the greater neuroscience community.  Because all of our infrastructure is open source, and deployed as Web-services, anybody---professional or citizen scientists---can use our services to learn about the brain.  Power users can modify our scripts to answer different questions, and developers could add additional capabilities for the community.  

# Results

All of the results presented herein are fundamentally reproducible; the data used to generate each table and figure panel can be reproduced by running a single Python script located in http://paper.neurodata.io.  Figures are made using R's ggplot2; each R script for generating the figures is also available from the above URL.  We have designed all of our supports to support three different types of data: images, annotations, and graphs.  Because the focus of this manuscript is on electron microscopy data, for which scant graphs are currently available, we focus the results on images and annotations.  However, see http://neurodata.io/graphs for more details about graph processing and analytics.


## Infrastructure

.TODO[@kl: make figs]

Our infrastructure is designed for fast reading and writing of terabytes of data.  Figure~\ref{fig:infra} shows the read and write speeds for our system, including writing both images and annotations.  Although we operate our own data cluster for the vast majority of our public facing data, we have built an Amazon Machine Instance (AMI) that is pre-configured with all the necessary software; all the results presented in this section are using that AMI, and are therefore wholly reproducible.


Importantly *NeuroData* is designed to easily enable reading or writing of arbitrarily sized image tiles or cubes using a single URL or Python call.  For example, to download the image in Figure~\ref{fig:infra}(A), one can simply type http://openconnecto.me/ocp/ca/kasthuri11/image/xy/0/9000,9500/12000,12500/50/ into any browser bar.  In contrast, if one desired to download an arbitrary image at an arbitrary zoom from data stored in files in a typical hierarchical folder system, one would have to write code to navigate to the right files, open them up, possibly downsample them, and stitch them back together.  Our infrastructure has built in support for all of that, significantly both speeding up and easing access to the data; Figure~\ref{fig:infra}(B) shows reading speed as a function of cube size, showing that reading speed is linear in the size of the data.    Because it is easy to implement different services for reading data into different file types, we have built support for a variety of exceptional 3rd party tools, to complement this ecosystem.  This includes ITK-SNAP \cite{XXX}, CATMAID \cite{XXX}, Viking \cite{XXX}, Knossos \cite{XXX}, VAST \cite{XXX}, and BigDataViewer \cite{XXX}.


Naively using the above described system, however, has the drawbacks just mentioned.  Therefore, we built *TileCache* which serves as a content distribution network.  More specifically, TileCache fetches the data and parses it into the appropriate format (e.g., a set of png files), and also pre-fetches data that it expects you will want next.  The implication is that for 1028x1028 frames, TileCache can load 60 frames per second in a streaming fashion (Figure~\ref{fig:infra}(C)).  In other words, using TileCache, Neurodata enables video rate data streaming.  And because TileCache is a stand-alone service, it can easily be deployed locally, thereby further eliminating the data transfer speeds to get data local to the machine on which you want the data.




Writing to our spatial database would be more time consuming than simply uploading to a file server, because we re-write the data into a format that is efficient for reading.  We combat that by using a distributed database management system (such as Cassandra), that automatically distributes writing jobs across nodes.  We further speed up writing by compressing the data using Blosc, a multi-threaded library.  The result is that we can write data in compressed cubes in time linear with the data size (Figure~\ref{fig:infra}(D)).  The implications are the writing a terabyte of data on a single node take XXX hours.  

Writing annotations is a bit more complicated than simply writing images because we store annotations as objects with associated metadata, such as what type of object is it (e.g., synapse), who labeled it, what time, with what confidence, etc. Because each write object is therefore relatively small, and write speeds are determined largely by the number of files to write, we built NeuroBlaze, which caches writes in memory, only writing to disk a single large file joining all the small writes.  This enables us to write  annotations extremely efficiently (Figure~\ref{fig:infra}(E)).  

<div class="container">
Figure 1: Reading and writing to the database must be sufficiently fast to not be the primary bottleneck.  (A) OCP can write images faster than most current data collection speeds, meaning that data can be sent directly to OCP without requiring a large local storage buffer.  (B) Reading images from OCP using the tilecache is sufficiently fast to view the data at video rates. (C) Unlike many other scientific big data efforts, neuroscience requires many fast annotation writes.  NeuroBlaze writes annotation objects much more efficiently than our image writing by caching the writes in memory. (D) Although current EM derived neurographs are relatively small, and there are few, we expect that to change dramatically in the coming years. Therefore, we have built infrastructure to support downloading a large number of large graphs efficiently.
</div>


## Databases

We used the above described infrastructure to build a number of image and annotation databases to support a variety of operations.  Table~\ref{tab:images}(A) lists the  public EM datasets that we are currently hosting, as well as several key properties of each.  Those properties that are required for efficient access of the data are stored in a Django data model, the remaining properties are stored in our custom Laboratory Information Management System called ndlims.  The image data in Table~\ref{tab:images}(A) comprise approximately 100 terabytes (TB) of image data .TODO[@j6: make jupyter notebook that builds latex compliant table to include, and sums up total storage].  By storing disparate datasets in the same format, anybody can access and analyze many different datasets with the same functionality and syntax.  

To complement the image databases, we also created an annotation database management system called RAMON \cite{GrayRoncal15b}.  We have used this infrastructure to store multiple manually and automatically generated dense volumetric annotations, listed in Table~\ref{tab:anno}(B).  

Supplementary Table~\ref{tab:API}(A) lists all the functionality currently supported on all of the image databases, and Supplementary Table~\ref{tab:API}(B) lists the functionality on all of the annotation databases.  Importantly, we have both MATLAB (called CAJAL) and Python (called ndio) bindings to support all of that functionality, without the user having to learn a new programming interface. 


<div class="container">
Table 1: List of all current (as of 12/31/15) Open Connectome Project Datasets.  Table 1A lists the *image* datasets and associated metadata.  Table 1B lists the volumetric *annotation* datasets, derived from a subset of the image datasets. Table 1C lists the *graph* datasets, derived from the annotation datasets.
</div>

## Explore

Interactively exploring these massive data requires tools that can automatically scale appropriately and efficiently.  For this reason, we have developed a number of *NeuroData Explorers*.   For images, we designed *Image Explorer* (IX; http://ix.neurodata.io).  It is the only visualization engine that has the following functionality: (i) loads data remotely from a server, (ii) works in Web-browsers, including mobile, (iii) natively supports volumetric annotations with associated metadata.  In particular, as Figure~\ref{fig:explorer}(A) demonstrates, one can visualize  electron microscopy data, and annotations on top (using one of many color blends), and also see the metadata associated with each annotation object.   Incidentally, IX also supports multiple color channels and time-series data, meaning that one can use it to overlay multiple disparate modalities.

To complement IX, we also built *Graph Explorer* (GX; http://gx.neurodata.io) and *Vector Explorer* (VX; http://vx.neurodata.io) (Figure~\ref{fig:explorer}(A) and (B)).  These two explorers enable users to upload either graphs or tabular data (where rows are subjects and columns are features, as in a csv file).  Both of these tools provide very basic functionality for graphs or tabular data that we find useful for exploring the relevant data types, including different normalization schemes, outlier detection, distributions of various features, and clustering.

<div class="container">
Figure 2: Exploring the data is a crucial first step, regardless of the data type, therefore, we have built data explorers for images, vectors, and graphs. (A) Image Explorer enables a Google Maps like interface to pan in X, Y, Z, and time, zoom, and overlay annotations, blend colors, and more.  (B) Graph Explorer facilitates computing various graph statistics, community detection, and plotting for arbitrary uploaded or generated graphs. (C) Vector Explorer allows the user to upload a csv file and do basic exploratory data analysis. 
</div>

## Pre-process

Pre-processing of data, a process that converts raw data into a format ready for parsing, is an incredibly important and tedious task for essentially any data analysis task.  In electron microscopy, we can divide pre-processing into three steps: (i) geometric or (ii) chromatic alterations, and (iii) registration to a template or atlas. A number of groups are actively developing very nice geometric pre-processing packages, including Terastitcher \cite{XXX} and Saalfeld's Fiji plug-in \cite{XXX}.  We therefore have focused our development efforts on the next two steps.  We built both a 2D and 3D chromatic pre-processing tool. The 2D tool, called *Distriubuted Multigrid Poisson* (DMG) addresses stitching artifacts from montaging multiple 2D tiles into a single large plane (Figure~\ref{fig:pre}(A)).  The 3D tool, called *Gradient Domain Fusion* (GDF) addresses histogram artifacts from different exposure times which can cause a strobe light effect across axial slices of the data (Figure~\ref{fig:pre}(B)).  Both tools were designed to stream the data efficiently, meaning that they can both operate on arbitrarily large data (Figure~\ref{fig:pre}(C)).  

After addressing both geometric and chromatic aberrations, it is often desirable to align the image stack to a reference image, possibly in a different modality.  *ndreg* is our Python tool that sub-serves that functionality.  Because there does not yet exist public data set with both EM and another modality to align, we have aligned a whole mouse brain CLARITY image with the Allen Reference Atlas (Figure~\ref{fig:pre}(D)).  Despite the fact that the two images were collected at different scales, and using different modalities, we are able to successfully and automatically align the two using *ndreg*.

<div class="container">
Figure 3: *NeuroData* provides fully automatic terascale tools for both 2D & 3D chromatic adjustments, as well as nonlinear volume registration.  (A) DMG Chromatic pre-processing adjusts for aberrations induces by data collection and/or geometric stitching. (Bi) 2D image before DMG. (Bii) 2D image after DMG.  (Biii) XZ projection of data before GDF. (Biv) XZ projection after GDF.  (Bv) Time per image size (red) and number of slices (black) on a particular AMI.  (C) Volume registration.
</div>

## Parse


Given that the data has been sufficiently pre-processed, we now proceed to parse the scene, assigning voxels with semantic labels.  Parsing the scene involves three steps: (i) manually labeling a subset of training data, (ii) training and evaluating various machine vision algorithms on appropriate performance metrics, and (iii) deploying the algorithm (and parameter) of choice at scale.  For manual labeling, we built *manno*, which essentially wraps ITK-SNAP by downloading image data from *NeuroData* in the appropriate format, uses ITK-SNAP to label a number of voxels, and then saves and uploads the annotations to the appropriate annotation database (with metadata; see Figure~\ref{fig:parse}(A) for an exemplar slice).  Given the training labels, *macho* uses ilastik to perform pixel level classification \cite{XXX}, and then custom morphological operations to detect objects.  We choose the operating point the maximized F1-score on a held-out dataset located in a volume separated from the training volume (Figure~\ref{fig:parse}(B)).  Finally, we deployed our chosen classifier on the full dataset, yielding ~10 million putative synapse detections (Figure~\ref{fig:parse}(C) shows approximately 1% of them).



<div class="container">
<div><img src="http://docs.neurodata.io/ndintro/images/rob2.png" alt="Drawing" style="height: 250px;"/></div>
Figure 4: 

</div>


## Analyze


A parsed scene is the data derivative upon which neuroscientific questions can be directly applied.  We focused on two particular questions.  First, a question previously addressed in the literature: To what extend does Peter's Rule hold amongst neurons in mammalian cortex.  To address this question



# Methods




# Captions


Supplementary Figure 1: Infrastructure supporting all components of the OCP data analysis pipeline.  (A) The OCP Data Cluster. (B) Software running on the backend. (C) Software running on the frontend. 







	</div>

	<script type="text/javascript">
        m = markdownit({
                html: true,
                linkify: true,
                typographer: true
        });
        document.getElementById('md-raw').innerHTML = m.render(
                document.getElementById('md-raw').innerHTML);
     </script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    </body>
</html>
 
